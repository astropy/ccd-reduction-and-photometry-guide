{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Estimation with photutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is background estimation?\n",
    "In order to most accurately do photometric analysis of celestial sources in image data, it is important to estimate and subtract the image background. Any astronomical image will have background noise, due to both detector effects and background emission from the night sky. This noise can be modeled as uniform, or as varying with position on the detector. \n",
    "\n",
    "The `photutils` package provides tools for estimating 2-dimensional background flux, which can then be subtracted from an image to ensure the most accurate photometry possible.\n",
    "\n",
    "### What does this tutorial include?\n",
    "This tutorial covers the basics of background estimation and subtraction, including the following methods:\n",
    "- Scalar Background Estimation\n",
    "- 2-D Background Estimation\n",
    "\n",
    "### Which data are used in this tutorial?\n",
    "We will be manipulating Hubble eXtreme Deep Field (XDF) data, which was collected using the Advanced Camera for Surveys (ACS) on Hubble between 2002 and 2012. The image we use here is the result of 1.8 million seconds (500 hours!) of exposure time, and includes some of the faintest and most distant galaxies that have ever been observed. \n",
    "\n",
    "Background subtraction is essential for accurate photometric analysis of astronomical data like the XDF.\n",
    "\n",
    "*The methods demonstrated here are available in narrative form within the `photutils.background` [documentation](http://photutils.readthedocs.io/en/stable/background.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "This notebook focuses on *global* background estimation. *Local* background subtraction with annulus apertures is demonstrated in FILL ME IN.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import packages that we will use to perform arithmetic functions and visualize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.nddata import CCDData\n",
    "import astropy.units as u\n",
    "from astropy.stats import sigma_clipped_stats, SigmaClip\n",
    "from astropy.visualization import ImageNormalize, LogStretch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "from photutils.segmentation import detect_threshold, detect_sources\n",
    "from photutils.utils import circular_footprint\n",
    "\n",
    "# Show plots in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some `matplotlib` parameters, such as title font size and the dpi, to make sure our plots look nice. To make it quick, we'll do this by loading a [style file shared with the other photutils tutorials](../photutils_notebook_style.mplstyle) into `pyplot`. We will use this style file for all the notebook tutorials. (See [here](https://matplotlib.org/users/customizing.html) to learn more about customizing `matplotlib`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('../photutils_notebook_style.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the introduction, we will be using Hubble eXtreme Deep Field (XDF) data. Since this file is too large to store on GitHub, we will just use `astropy` to directly download the file from the STScI archive: https://archive.stsci.edu/prepds/xdf/ \n",
    "\n",
    "(Generally, the best package for web queries of astronomical data is [Astroquery](https://astroquery.readthedocs.io/en/latest/); however, the dataset we are using is a High Level Science Product (HLSP) and thus is not located within a catalog that could be queried with Astroquery.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.stsci.edu/pub/hlsp/xdf/hlsp_xdf_hst_acswfc-60mas_hudf_f435w_v1_sci.fits'\n",
    "with fits.open(url) as hdulist:\n",
    "    hdulist.info()\n",
    "    data = hdulist[0].data\n",
    "    header = hdulist[0].header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify data by adding gradient\n",
    "\n",
    "For the purposes of this notebook example, we're going to add a linear background effect from the top to the bottom of these data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data == 0\n",
    "n_data_pixels = len(data[~mask])\n",
    "background = np.linspace(-1e-4, 5e-4, num=n_data_pixels)\n",
    "\n",
    "modified_data = np.copy(data)\n",
    "modified_data[~mask] += background[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data representation\n",
    "\n",
    "Throughout this notebook, we are going to store our images in Python using a `CCDData` object (see [Astropy documentation](http://docs.astropy.org/en/stable/nddata/index.html#ccddata-class-for-images)), which contains a `numpy` array in addition to metadata such as uncertainty, masks, or units. In this case, each image has units electrons (counts) per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = u.electron / u.s\n",
    "xdf_image = CCDData(modified_data, unit=unit, meta=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "# Set up the normalization and colormap. The values of vmin and vmax are specific to this image\n",
    "norm_image = ImageNormalize(vmin=1e-4, vmax=5e-2, stretch=LogStretch())\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Plot the data, with pixels masked as appropriate\n",
    "fitsplot = ax1.imshow(np.ma.masked_where(xdf_image.mask, xdf_image),\n",
    "                      norm=norm_image, cmap=cmap)\n",
    "\n",
    "# Define the colorbar\n",
    "cbar = plt.colorbar(fitsplot, fraction=0.046, pad=0.04, ticks=LogLocator())\n",
    "\n",
    "def format_colorbar(bar):\n",
    "    # Add minor tickmarks\n",
    "    bar.ax.yaxis.set_minor_locator(LogLocator(subs=range(1, 10)))\n",
    "\n",
    "    # Force the labels to be displayed as powers of ten and only at exact powers of ten\n",
    "    bar.ax.set_yticks([1e-4, 1e-3, 1e-2])\n",
    "    labels = [f'$10^{{{pow:.0f}}}$' for pow in np.log10(bar.ax.get_yticks())]\n",
    "    bar.ax.set_yticklabels(labels)\n",
    "\n",
    "format_colorbar(cbar)\n",
    "\n",
    "# Define labels\n",
    "cbar.set_label(r'Flux Count Rate ({})'.format(xdf_image.unit.to_string('latex')),\n",
    "               rotation=270, labelpad=30)\n",
    "ax1.set_xlabel('X (pixels)')\n",
    "ax1.set_ylabel('Y (pixels)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask non-data portions of array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that a large portion of the data is equal to zero. The data we are using is a reduced mosaic that combines many different exposures, and that has been rotated such that not all of the array holds data. \n",
    "\n",
    "We want to **mask** out the non-data portions of the image array, so all of those pixels that have a value of zero don't interfere with our statistics and analyses of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mask\n",
    "xdf_image.mask = xdf_image.data == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot the mask\n",
    "ax1.imshow(xdf_image.mask, cmap='Greys')\n",
    "ax1.set_xlabel('X (pixels)')\n",
    "ax1.set_ylabel('Y (pixels)')\n",
    "ax1.set_title('Mask')\n",
    "\n",
    "# Plot the masked data\n",
    "fitsplot = ax2.imshow(np.ma.masked_where(xdf_image.mask, xdf_image),\n",
    "                      norm=norm_image, cmap=cmap)\n",
    "\n",
    "# Define the colorbar and fix the labels\n",
    "cbar_ax = fig.add_axes([1, 0.09, 0.03, 0.87])\n",
    "cbar = fig.colorbar(fitsplot, cbar_ax, ticks=LogLocator())\n",
    "\n",
    "format_colorbar(cbar)\n",
    "\n",
    "cbar.set_label(r'Flux Count Rate ({})'.format(xdf_image.unit.to_string('latex')),\n",
    "               rotation=270, labelpad=30)\n",
    "ax2.set_xlabel('X (pixels)')\n",
    "ax2.set_title('Masked Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left we have plotted this mask, which has a value of 1 (or True) shown in black where the data is bad, and 0 (or False) shown in white where the data is good. \n",
    "\n",
    "After the mask is applied to the data (on the right above) the data values \"behind\" the masked values are shown in white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform scalar background estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data are properly masked, we can calculate some basic statistical values to do a scalar estimation of the image background. \n",
    "\n",
    "By \"scalar estimation\", we mean the calculation of a single value (such as the mean or median) to represent the value of the background for our entire two-dimensional dataset. This is in contrast to a two-dimensional background, where the estimated background is represented as an array of values that can vary spatially with the dataset. We will calculate a 2D background in the upcoming section.\n",
    "\n",
    "### Calculate scalar background value\n",
    "\n",
    "Here we will calculate the mean, median, and mode of the dataset using sigma clipping. With sigma clipping, the data is iteratively clipped to exclude data points outside of a certain sigma (standard deviation), thus removing some of the noise from the data before determining statistical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics with masking\n",
    "mean, median, std = sigma_clipped_stats(xdf_image.data, sigma=3.0, maxiters=5, mask=xdf_image.mask)\n",
    "\n",
    "# Calculate statistics without masking\n",
    "stats_nomask = sigma_clipped_stats(xdf_image.data, sigma=3.0, maxiters=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate scalar bkdg value without sigma clipping\n",
    "\n",
    "Here we will calculate the mean, median, and mode of the dataset without using sigma clipping. \n",
    "\n",
    "As above, we do this first taking into account the mask and then ignoring the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_noclip_mask = np.mean(xdf_image.data[~xdf_image.mask])\n",
    "median_noclip_mask = np.median(xdf_image.data[~xdf_image.mask])\n",
    "\n",
    "mean_noclip_nomask = np.mean(xdf_image.data)\n",
    "median_noclip_nomask = np.median(xdf_image.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask sources, then calculate scalar background\n",
    "\n",
    "As discussed in [FILL IN LINK](FILL IN LINK), the most accurate estimate of the scalar background is obtained when the sources are masked first. The cell below does that procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up sigma clipping\n",
    "sigma_clip = SigmaClip(sigma=3.0, maxiters=10)\n",
    "\n",
    "threshold = detect_threshold(xdf_image.data, nsigma=2.0, sigma_clip=sigma_clip, mask=xdf_image.mask)\n",
    "segment_img = detect_sources(xdf_image.data, threshold, npixels=10, mask=xdf_image.mask)\n",
    "footprint = circular_footprint(radius=10)\n",
    "\n",
    "# Make the source mask a circle of radius 10 around each detected source\n",
    "source_mask = segment_img.make_source_mask(footprint=footprint)\n",
    "\n",
    "# Combine the source mask with the data mask\n",
    "full_mask = source_mask | xdf_image.mask\n",
    "\n",
    "# Compute the stats\n",
    "source_mask_clip_mean, source_mask_clip_med, source_mask_clip_std  = sigma_clipped_stats(data, sigma=3, mask=full_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what difference does this sigma clipping make? And how important is masking, anyway? Let's visualize these statistics to get an idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "# Plot histograms of the data\n",
    "flux_range = (-.5e-3, 1.5e-3)\n",
    "ax1.hist(xdf_image.data[~xdf_image.mask], bins=100, range=flux_range)\n",
    "ax2.hist(xdf_image.data[~xdf_image.mask], bins=100, range=flux_range)\n",
    "\n",
    "# Plot lines for each kind of mean\n",
    "ax1.axvline(mean, label='Non-data masked and Clipped', c='C1', ls='-.', lw=3)\n",
    "ax1.axvline(mean_noclip_mask, label='Masked', c='C2', lw=3)\n",
    "ax1.axvline(stats_nomask[0], label='Clipped', c='C3', ls=':', lw=5)\n",
    "ax1.axvline(source_mask_clip_mean, label='Sources masked and clipped', c='C4', lw=3)\n",
    "ax1.axvline(mean_noclip_nomask, label='Neither', c='C5', ls='--', lw=3)\n",
    "\n",
    "ax1.set_xlim(flux_range)\n",
    "ax1.set_xlabel(r'Flux Count Rate ({})'.format(xdf_image.unit.to_string('latex')), fontsize=14)\n",
    "ax1.set_ylabel('Frequency', fontsize=14)\n",
    "ax1.set_title('Effect of Sigma-Clipping \\n and Masking on Mean', fontsize=16)\n",
    "\n",
    "# Plot lines for each kind of median\n",
    "# Note: use np.ma.median rather than np.median for masked arrays\n",
    "ax2.axvline(median, label='Non-data masked and Clipped', c='C1', ls='-.', lw=3)\n",
    "ax2.axvline(median_noclip_mask, label='Masked', c='C2', lw=3)\n",
    "ax2.axvline(source_mask_clip_med, label='Sources masked and clipped', c='C4', lw=3)\n",
    "ax2.axvline(stats_nomask[1], label='Clipped', c='C3', ls=':', lw=5)\n",
    "ax2.axvline(median_noclip_nomask, label='Neither', c='C5', ls='--', lw=3)\n",
    "\n",
    "\n",
    "ax2.set_xlim(flux_range)\n",
    "ax2.set_xlabel(r'Flux Count Rate ({})'.format(xdf_image.unit.to_string('latex')), fontsize=14)\n",
    "ax2.set_title('Effect of Sigma-Clipping \\n and Masking on Median', fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "ax1.legend(fontsize=11, loc='lower center', bbox_to_anchor=(1.1, -0.55), ncol=2, handlelength=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from simply looking at the distribution of the data, it is pretty easy to see how sigma-clipping and masking improve the calculation of the mean and median: the masked and sigma-clipped values are closest to the center of the distribution in both cases. It's also worthwhile to note that the median does a better job even without masking or clipping! Finally, note that using the median with sigma clipping gets essentially the same results as masking the sources along with sigma clipping.\n",
    "\n",
    "### Subtract scalar background value\n",
    "\n",
    "But enough looking at numbers, let's actually remove the background from the data. By using the `subtract()` method of the `CCDData` class, we can subtract the mean background while maintaining the metadata and mask of our original CCDData object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the scalar background subtraction, maintaining metadata, unit, and mask\n",
    "xdf_scalar_bkgdsub = xdf_image.subtract(mean * u.electron / u.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot the original data\n",
    "fitsplot = ax1.imshow(np.ma.masked_where(xdf_image.mask, xdf_image), norm=norm_image)\n",
    "ax1.set_xlabel('X (pixels)')\n",
    "ax1.set_ylabel('Y (pixels)')\n",
    "ax1.set_title('Original Data')\n",
    "\n",
    "# Plot the subtracted data\n",
    "fitsplot = ax2.imshow(np.ma.masked_where(xdf_scalar_bkgdsub.mask, xdf_scalar_bkgdsub), norm=norm_image)\n",
    "ax2.set_xlabel('X (pixels)')\n",
    "ax2.set_title('Scalar Background-Subtracted Data')\n",
    "\n",
    "# Define the colorbar...\n",
    "cbar_ax = fig.add_axes([1, 0.09, 0.03, 0.87])\n",
    "\n",
    "cbar = fig.colorbar(fitsplot, cbar_ax, ticks=LogLocator())\n",
    "\n",
    "format_colorbar(cbar)\n",
    "\n",
    "cbar.set_label(r'Flux Count Rate ({})'.format(xdf_image.unit.to_string('latex')),\n",
    "               rotation=270, labelpad=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both plots above use the same normalization scheme, represented by the colorbar on the right. That is to say, if two pixels have the same color in both arrays, they have the same value.\n",
    "\n",
    "That looks better! You can tell that the background is darker, especially in the top corner. However, the background still does not seem to be completely removed. In this case, the background varies spatially; it is two-dimensional. Thankfully, `photutils` includes functions to remove background like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform 2-D background estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Background2D` class allows users to model 2-dimensional backgrounds, by calculating the mean or median in small boxes, and smoothing these boxes to reconstruct a continuous 2D background. The class includes the following arguments/attributes:\n",
    "* **`box_size`** &mdash; the size of the boxes used to calculate the background. This should be larger than individual sources, yet still small enough to encompass changes in the background.\n",
    "* **`filter_size`** &mdash; the size of the median filter used to smooth the final 2D background. The dimension should be odd along both axes.\n",
    "* **`filter_threshold`** &mdash; threshold below which the smoothing median filter will not be applied.\n",
    "* **`sigma_clip`** &mdash; an ` astropy.stats.SigmaClip` object that is used to specify the sigma and number of iterations used to sigma-clip the data before background calculations are performed.\n",
    "* **`bkg_estimator`** &mdash; the method used to perform the background calculation in each box (mean, median, SExtractor algorithm, etc.).\n",
    "\n",
    "For this example, we will use the `MeanBackground` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.background import Background2D, MeanBackground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_clip = SigmaClip(sigma=3., maxiters=5)\n",
    "bkg_estimator = MeanBackground()\n",
    "bkg = Background2D(xdf_image, box_size=200, filter_size=(9, 9), mask=xdf_image.mask,\n",
    "                   sigma_clip=sigma_clip, bkg_estimator=bkg_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what does this 2D background look like? Where were the boxes placed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "# Plot the background\n",
    "fitsplot = ax1.imshow(np.ma.masked_where(xdf_image.mask, bkg.background.data ), norm=norm_image)\n",
    "\n",
    "# Plot the meshes\n",
    "bkg.plot_meshes(outlines=True, color='lightgrey')\n",
    "\n",
    "# Define the colorbar\n",
    "cbar = plt.colorbar(fitsplot, fraction=0.046, pad=0.04, ticks=LogLocator())\n",
    "\n",
    "format_colorbar(cbar)\n",
    "\n",
    "# Define labels\n",
    "cbar.set_label(r'Flux Count Rate ({})'.format(xdf_image.unit.to_string('latex')),\n",
    "               rotation=270, labelpad=30)\n",
    "ax1.set_xlabel('X (pixels)')\n",
    "ax1.set_ylabel('Y (pixels)')\n",
    "ax1.set_title('2D Estimated Background');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that not all areas of the background array have mesh boxes over them (look for those boxes that do not have a `+`). If you compare this background array with the original data, you'll see that these un-boxed areas contain particularly bright sources, and thus are not being included in the background estimate .\n",
    "\n",
    "And how does the data look if we use this background subtraction method (again maintaining the attributes of the CCDData object)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 2D background subtraction, maintaining metadata, unit, and mask\n",
    "xdf_2d_bkgdsub = xdf_image.subtract(bkg.background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot the scalar-subtracted data\n",
    "fitsplot = ax1.imshow(np.ma.masked_where(xdf_scalar_bkgdsub.mask, xdf_scalar_bkgdsub), norm=norm_image)\n",
    "cbar.set_label(r'Flux Count Rate ({})'.format(xdf_image.unit.to_string('latex')),\n",
    "               rotation=270, labelpad=30)\n",
    "ax1.set_ylabel('Y (pixels)')\n",
    "ax1.set_xlabel('X (pixels)')\n",
    "ax1.set_title('Scalar Background-Subtracted Data')\n",
    "\n",
    "# Plot the 2D-subtracted data\n",
    "fitsplot = ax2.imshow(np.ma.masked_where(xdf_2d_bkgdsub.mask, xdf_2d_bkgdsub), norm=norm_image)\n",
    "ax2.set_xlabel('X (pixels)')\n",
    "ax2.set_title('2D Background-Subtracted Data')\n",
    "\n",
    "# Define the colorbar...\n",
    "cbar_ax = fig.add_axes([1, 0.09, 0.03, 0.87])\n",
    "\n",
    "cbar = fig.colorbar(fitsplot, cbar_ax, ticks=LogLocator())\n",
    "cbar.ax.yaxis.set_minor_locator(LogLocator(subs=range(1, 10)))\n",
    "\n",
    "# ...and force the labels to be displayed as powers of ten\n",
    "cbar.ax.set_yticks([1e-4, 1e-3, 1e-2])\n",
    "labels = [f'$10^{{{pow:.0f}}}$' for pow in np.log10(cbar.ax.get_yticks())]\n",
    "cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "cbar.set_label(r'Flux Count Rate ({})'.format(xdf_image.unit.to_string('latex')),\n",
    "               rotation=270, labelpad=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how much more even the 2D background-subtracted image looks; especially the difference between these two images in the bottom corner and top corner. This makes sense, as the background that `Background2D` identified was a gradient from the top corner down to the bottom!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The `photutils` package provides a powerful tool in the `Background2D` class, allowing users to easily estimate and subtract spatially variant background signals from their data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
